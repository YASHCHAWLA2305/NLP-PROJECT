{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "77365c57-3b67-4aac-a4a8-a1c0825f84eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# @@@@@@@@@@@@@@@@@@@ DATA LOADING @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load data using raw string literal\n",
    "df = pd.read_excel(r'C:\\Users\\Sham Sunder Chawla\\Desktop\\Big Data\\15. Capstone Case Study - NLP- Woman Clothing E-Commerce Platform (2)\\Womens Clothing Reviews Data.xlsx')\n",
    "\n",
    "\n",
    "\n",
    "# @@@@@@@@@@@@@@@@@@@ DATA CLEANING @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "\n",
    "# DATA CLEANING-Step 1: \n",
    "\n",
    "# Check for missing values (NaNs) in each column\n",
    "\n",
    "missing_values_per_column = df.isnull().sum()\n",
    "\n",
    "# Check for empty strings in each column\n",
    "empty_strings_per_column = (df == '').sum()\n",
    "\n",
    "# Combine the counts of missing values and empty strings per column\n",
    "total_missing_or_empty_per_column = missing_values_per_column + empty_strings_per_column\n",
    "\n",
    "print(\"Total Rows with Missing Values or Empty Strings in Each Column:\")\n",
    "print(total_missing_or_empty_per_column)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Drop rows with missing values or empty strings in specific columns\n",
    "\n",
    "columns_to_check = ['Category', 'Subcategory1', 'SubCategory2']\n",
    "df = df.dropna(subset=columns_to_check, how='any')\n",
    "\n",
    "# Reset index if you want consecutive integer indices after dropping rows\n",
    "\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# DATA CLEANING-Step 2: \n",
    "\n",
    "\n",
    "# Iterate over rows and replace empty/missing values in ColumnA with values from ColumnB\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    if pd.isna(row['Review Text']) or row['Review Text'] == '':\n",
    "        df.at[index, 'Review Text'] = row['Review Title']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(df.head())\n",
    "\n",
    "# Check the shape of the DataFrame\n",
    "print(\"Shape of DataFrame:\", df.shape)\n",
    "\n",
    "\n",
    "# @@@@@@@@@@@@@@@ text preprocessing @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n",
    "\n",
    "# Step 1 : LOWERCASING\n",
    "\n",
    "\n",
    "# Lowercase the 'Text' column\n",
    "df['Review Text_lowercase'] = df['Review Text'].str.lower()\n",
    "\n",
    "\n",
    "# Drop the column Review text (e.g., 'ColumnC')\n",
    "\n",
    "df.drop(columns=['Review Text'], inplace=True)\n",
    "\n",
    "\n",
    "#2 Step 2 : Tokenize \n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "  \n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "\n",
    "# Tokenize the 'Review Text_lowercase' column\n",
    "df['Review Text_Tokens'] = df['Review Text_lowercase'].apply(word_tokenize)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Step 3 : Remove Punctuations\n",
    "\n",
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "\n",
    "\n",
    "# Function to remove punctuation from a list of tokens\n",
    "\n",
    "def remove_punctuation(tokens):\n",
    "    return [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
    "\n",
    "# Remove punctuation from each list of tokens in the 'Review Text_Tokens' column\n",
    "\n",
    "df['Review Text_Tokens'] = df['Review Text_Tokens'].apply(remove_punctuation)\n",
    "\n",
    "# Step 4 : Remove Stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# Function to remove stop words from a list of tokens\n",
    "\n",
    "    def remove_stopwords(tokens):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "# Remove stop words from each list of tokens in the 'Review Text_Tokens' column\n",
    "\n",
    "df['Review Text_Tokens'] = df['Review Text_Tokens'].apply(remove_stopwords)\n",
    "\n",
    "\n",
    "\n",
    "# Step 5 : Stemming/Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "# Function to perform lemmatization on a list of tokens\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Lemmatize tokens in the 'Review Text_Tokens' column\n",
    "\n",
    "df['Review Text_Tokens'] = df['Review Text_Tokens'].apply(lemmatize_tokens)\n",
    "\n",
    "\n",
    "# Step 6 : Handling Contractions and Abbreviations\n",
    "\n",
    "!pip install contractions\n",
    "\n",
    "import pandas as pd\n",
    "import contractions\n",
    "\n",
    "\n",
    "\n",
    "# Function to expand contractions in tokenized text\n",
    "\n",
    "def expand_contractions(tokens):\n",
    "    expanded_tokens = []\n",
    "    for token in tokens:\n",
    "        expanded_token = contractions.fix(token)\n",
    "        expanded_tokens.extend(expanded_token.split())  # Split if contraction expands to multiple words\n",
    "    return expanded_tokens\n",
    "\n",
    "# Apply contraction expansion to the 'Review Text_Tokens' column\n",
    "\n",
    "df['Review Text_Tokens'] = df['Review Text_Tokens'].apply(expand_contractions)\n",
    "\n",
    "\n",
    "\n",
    "# Question a) Performing exploratory analysis on the data to understand the patterns\n",
    "\n",
    "\n",
    "# Flatten the list of tokenized words (create one list from multiple lists)\n",
    "\n",
    "all_tokens = [token for sublist in df['Review Text_Tokens'] for token in sublist]\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(all_tokens)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the top N most common words\n",
    "N = 10\n",
    "top_words = word_freq.most_common(N)\n",
    "labels, values = zip(*top_words)\n",
    "\n",
    "plt.bar(labels, values)\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Top {} Most Common Words'.format(N))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Question B: Perform text mining tasks to understand what most frequent words are using for positive sentiment and negative sentiment. Create word clouds for the positive & negative reviews separately.\n",
    "\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "\"\"\" Convert between the Penn Treebank tags used by NLTKâ€™s pos_tag() to simple WordNet tags \"\"\"\n",
    "\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('N'):\n",
    "        return wn.NOUN\n",
    "    if tag.startswith('V'):\n",
    "        return wn.VERB\n",
    "    if tag.startswith('J'):\n",
    "        return wn.ADJ\n",
    "    if tag.startswith('R'):\n",
    "        return wn.ADV\n",
    "    return None\n",
    "\n",
    "\"\"\" Returns the net sentiment score for a word based on its part of speech tag \"\"\"\n",
    "\n",
    "def get_sentiment(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return 0\n",
    "\n",
    "    lemma = wn.morphy(word, wn_tag)\n",
    "    if not lemma:\n",
    "        return 0\n",
    "\n",
    "    synsets = wn.synsets(word, wn_tag)\n",
    "    if not synsets:\n",
    "        return 0\n",
    "\n",
    "    # Sum up all the sentiment scores of all synsets to get an average\n",
    "    \n",
    "    sentiment = [swn.senti_synset(synset.name()) for synset in synsets]\n",
    "    positive = sum([s.pos_score() for s in sentiment]) / len(sentiment)\n",
    "    negative = sum([s.neg_score() for s in sentiment]) / len(sentiment)\n",
    "    return positive - negative\n",
    "\n",
    "def sentiment_score(tokens):\n",
    "    \"\"\" Compute the sentiment score for pre-tokenized text \"\"\"\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "    scores = [get_sentiment(word, tag) for word, tag in pos_tags]\n",
    "    scores = [score for score in scores if score != 0]  # Remove zero scores\n",
    "    if not scores:\n",
    "        return 0\n",
    "    return sum(scores)\n",
    "\n",
    "# Applying it to the DataFrame\n",
    "df['overall_sentiment'] = df['Review Text_Tokens'].apply(sentiment_score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# create wordcloud\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Function to generate word clouds\n",
    "def generate_wordcloud(text, title):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(text))\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "\n",
    "# Gather tokens by sentiment orientation\n",
    "positive_tokens = []\n",
    "negative_tokens = []\n",
    "\n",
    "for tokens in df['Review Text_Tokens']:\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    for word, tag in pos_tags:\n",
    "        orientation = get_sentiment_orientation(word, tag)\n",
    "        if orientation == 'positive':\n",
    "            positive_tokens.append(word)\n",
    "        elif orientation == 'negative':\n",
    "            negative_tokens.append(word)\n",
    "\n",
    "# Generate word clouds for positive and negative tokens\n",
    "generate_wordcloud(positive_tokens, 'Positive Sentiment Words')\n",
    "generate_wordcloud(negative_tokens, 'Negative Sentiment Words')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Question C : Understand sentiment among the customers on the different categories, sub categories,products by location and age group\n",
    "\n",
    "# Aggregating sentiment scores by various dimensions\n",
    "\n",
    "aggregated_scores = df.groupby(['Category', 'Subcategory1', 'SubCategory2', 'Location', 'Channel']).agg({\n",
    "    'overall_sentiment': ['mean', 'count', 'std']  # You can adjust the aggregations as needed\n",
    "}).reset_index()\n",
    "aggregated_scores.columns = [' '.join(col).strip() for col in aggregated_scores.columns.values]\n",
    "\n",
    "\n",
    "# Question D : Perform predictive analytics to understand the drivers of customers who are recommending the products\n",
    "\n",
    "\n",
    "# drivers of recomendation=1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your existing DataFrame\n",
    "\n",
    "# Filter the DataFrame to include only rows where Recommended_Flag is 1\n",
    "recommended_df = df[df['Recommend Flag'] == 1]\n",
    "\n",
    "\n",
    "\n",
    "!pip install pandas nltk afinn\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, sentiwordnet as swn\n",
    "from afinn import Afinn\n",
    "\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "import string\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "afinn = Afinn()\n",
    "\n",
    "def find_positive_afinn(tokens):\n",
    "    positive_words = [word for word in tokens if afinn.score(word) > 0]\n",
    "    return positive_words\n",
    "\n",
    "recommended_df['positive_afinn'] = recommended_df['Review Text_Tokens'].apply(find_positive_afinn)\n",
    "\n",
    "\n",
    "recommended_df['positive_combined'] = recommended_df.apply(lambda row: list(set(row['positive_afinn'] + row['positive_sentiwordnet'])), axis=1)\n",
    "\n",
    "# Question 5:Create topics and understand themes behind the topics by performing topic mining.\n",
    "\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import pyLDAvis\n",
    "import pandas as pd\n",
    "\n",
    "# Install pyLDAvis if not already installed\n",
    "\n",
    "import sys\n",
    "get_ipython().system('{sys.executable} -m pip install pyLDAvis gensim pandas')\n",
    "\n",
    "\n",
    "# Ensure that the column 'Review Text_Tokens' contains lists of tokens\n",
    "\n",
    "reviews_tokenized = df['Review Text_Tokens'].tolist()\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "\n",
    "dictionary = corpora.Dictionary(reviews_tokenized)\n",
    "\n",
    "# Filter out extremes to limit the number of features\n",
    "\n",
    "dictionary.filter_extremes(no_below=10, no_above=0.5)\n",
    "\n",
    "# Create the Bag of Words corpus\n",
    "\n",
    "corpus = [dictionary.doc2bow(review) for review in reviews_tokenized]\n",
    "\n",
    "# Set parameters for the LDA model\n",
    "num_topics = 5\n",
    "lda_model = gensim.models.LdaModel(\n",
    "    corpus,\n",
    "    num_topics=num_topics,\n",
    "    id2word=dictionary,\n",
    "    passes=15,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=reviews_tokenized, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score: {coherence_lda}')\n",
    "\n",
    "# Print the topics\n",
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(f'Topic: {idx} \\nWords: {topic}\\n')\n",
    "\n",
    "# Prepare the visualization\n",
    "vis = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "pyLDAvis.save_html(vis, 'lda_visualization.html')\n",
    "\n",
    "# To view the visualization\n",
    "pyLDAvis.display(vis)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#classification model\n",
    "\n",
    "get_ipython().system('pip install pandas-profiling')\n",
    "\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "\n",
    "# Generate the EDA report\n",
    "profile = ProfileReport(df, title='Pandas Profiling Report', explorative=True)\n",
    "\n",
    "# Save the report to a file\n",
    "profile.to_file(\"eda_report.html\")\n",
    "\n",
    "\n",
    "\n",
    "# drop irrelevant variables\n",
    "\n",
    "columns_to_average=[\"Product ID\",\"Review Title\"]\n",
    "df=df.drop(columns_to_average,axis=1)\n",
    "\n",
    "\n",
    "# In[110]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# List of columns to be one-hot encoded\n",
    "columns_to_encode = ['Category','Subcategory1','SubCategory2','Location','Channel']\n",
    "\n",
    "# Perform one-hot encoding\n",
    "one_hot_encoder = OneHotEncoder(sparse=False, drop=None)\n",
    "encoded_features = one_hot_encoder.fit_transform(df[columns_to_encode])\n",
    "\n",
    "# Create a DataFrame with the encoded features\n",
    "encoded_df = pd.DataFrame(encoded_features, columns=one_hot_encoder.get_feature_names_out(columns_to_encode))\n",
    "\n",
    "# Drop the original categorical columns and concatenate the encoded columns\n",
    "df = df.drop(columns=columns_to_encode).reset_index(drop=True)\n",
    "df = pd.concat([df, encoded_df], axis=1)\n",
    "\n",
    "\n",
    "# In[112]:\n",
    "\n",
    "\n",
    "# drop irrelevant variables\n",
    "\n",
    "columns_to_average=['Review Text_lowercase','Review Text_Tokens']\n",
    "df=df.drop(columns_to_average,axis=1)\n",
    "\n",
    "\n",
    "# In[121]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Define input features (X) and target variable (y)\n",
    "X = df.drop(columns=['Recommend Flag'])\n",
    "y = df['Recommend Flag']\n",
    "\n",
    "\n",
    "# Split your data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "# In[122]:\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Create and train the logistic regression model\n",
    "logistic_regression_model = LogisticRegression()\n",
    "logistic_regression_model.fit(X_train, y_train)\n",
    "\n",
    "# Step 4: Evaluate the model\n",
    "y_pred = logistic_regression_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "354571b1-ff2f-4dbc-9d94-8edf7dc647b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data using raw string literal\n",
    "df = pd.read_excel(r'C:\\Users\\Sham Sunder Chawla\\Desktop\\Big Data\\15. Capstone Case Study - NLP- Woman Clothing E-Commerce Platform (2)\\Womens Clothing Reviews Data.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61e648b8-4cae-4b6a-84dc-9152e14f9ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23486 entries, 0 to 23485\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Product ID      23486 non-null  int64 \n",
      " 1   Category        23472 non-null  object\n",
      " 2   Subcategory1    23472 non-null  object\n",
      " 3   SubCategory2    23472 non-null  object\n",
      " 4   Location        23486 non-null  object\n",
      " 5   Channel         23486 non-null  object\n",
      " 6   Customer Age    23486 non-null  int64 \n",
      " 7   Review Title    19676 non-null  object\n",
      " 8   Review Text     22641 non-null  object\n",
      " 9   Rating          23486 non-null  int64 \n",
      " 10  Recommend Flag  23486 non-null  int64 \n",
      "dtypes: int64(4), object(7)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd8de49-a4c3-499d-823f-df8371d7afdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Review Title\"]=df[\"Review Title\"].fillna(\"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "585baa75-ea39-4ef9-8727-1ed66275859e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2723900923.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mmonth of current date=Month.today()\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0417a9-c3e4-4eb5-b5c3-5130749d6f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
